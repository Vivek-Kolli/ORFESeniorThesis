{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNbQUnFyLwRKJl5DfHljhM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhyMAcHAlofq"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## GOOD RESULTS WITH GENERALIZATION AND L2 THING\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/Full Dataset (Imputed Values, -1s removed).csv')  # Update the path as necessary\n",
        "\n",
        "# Filter for draft years 2017-2023\n",
        "# This is getting data for 2017-2023\n",
        "data_filtered = data[data['Draft Year'].between(2017, 2023)]\n",
        "#data_filtered = data\n",
        "\n",
        "# Select only numerical columns\n",
        "numerical_data_filtered = data_filtered.select_dtypes(include=['number'])\n",
        "\n",
        "# Impute missing values with mean\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "numerical_data_imputed = imputer.fit_transform(numerical_data_filtered)\n",
        "numerical_data_imputed_df = pd.DataFrame(numerical_data_imputed, columns=numerical_data_filtered.columns)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(numerical_data_imputed_df)\n",
        "scaled_data_df = pd.DataFrame(scaled_data, columns=numerical_data_imputed_df.columns)\n",
        "\n",
        "# Prepare your features and target variable, excluding 'Draft Year' from features for the model\n",
        "features = scaled_data_df.drop(columns=['WS', 'WS/48', 'BPM', 'VORP/48'])\n",
        "targets_ws48 = scaled_data_df['WS/48']\n",
        "\n",
        "def create_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(input_shape,), kernel_regularizer=l2(0.01)),  # L2 regularization added\n",
        "        Dropout(0.5),  # Dropout added\n",
        "        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),  # L2 regularization added\n",
        "        Dropout(0.5),  # Dropout added\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Convert features and targets to numpy arrays for cross-validation\n",
        "X = features.to_numpy()\n",
        "y = targets_ws48.to_numpy().ravel()\n",
        "\n",
        "# Define Repeated K-Fold Cross Validator\n",
        "rkf = RepeatedKFold(n_splits=10, n_repeats=5, random_state=42)\n",
        "\n",
        "mse_scores = []\n",
        "rmse_scores = []\n",
        "r2_scores = []\n",
        "train_mse_scores = []\n",
        "train_rmse_scores = []\n",
        "train_r2_scores = []\n",
        "\n",
        "# Cross-validation loop\n",
        "for train_index, test_index in rkf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    model = create_model(input_shape=X_train.shape[1])\n",
        "    # Epochs are how many times over you go through\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0)\n",
        "\n",
        "    # Training set evaluation\n",
        "    train_predictions = model.predict(X_train)\n",
        "    train_mse = mean_squared_error(y_train, train_predictions.flatten())\n",
        "    train_rmse = np.sqrt(train_mse)\n",
        "    train_r2 = r2_score(y_train, train_predictions.flatten())\n",
        "    train_mse_scores.append(train_mse)\n",
        "    train_rmse_scores.append(train_rmse)\n",
        "    train_r2_scores.append(train_r2)\n",
        "\n",
        "    # Testing set evaluation\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions.flatten())\n",
        "    rmse = np.sqrt(mse)\n",
        "    r_squared = r2_score(y_test, predictions.flatten())\n",
        "    mse_scores.append(mse)\n",
        "    rmse_scores.append(rmse)\n",
        "    r2_scores.append(r_squared)\n",
        "\n",
        "# Compute average metrics for both training and testing sets\n",
        "print(f\"Average Train MSE: {np.mean(train_mse_scores)}\")\n",
        "print(f\"Average Train RMSE: {np.mean(train_rmse_scores)}\")\n",
        "print(f\"Average Train R-squared: {np.mean(train_r2_scores)}\")\n",
        "print(f\"Average Test MSE: {np.mean(mse_scores)}\")\n",
        "print(f\"Average Test RMSE: {np.mean(rmse_scores)}\")\n",
        "print(f\"Average Test R-squared: {np.mean(r2_scores)}\")"
      ],
      "metadata": {
        "id": "i1E6B-Xx07r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## GOOD RESULTS WITH Generalization, L2 and simplifying model through increased dropout rate and less layers (FINAL MODEL)\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/Quantitative Stats (College, NBA, Combine).csv')  # Update the path as necessary\n",
        "\n",
        "# Filter for draft years 2017-2023\n",
        "# This is getting data for 2017-2023\n",
        "#data_filtered = data[data['Draft Year'].between(2017, 2023)]\n",
        "data_filtered = data\n",
        "\n",
        "# Select only numerical columns\n",
        "numerical_data_filtered = data_filtered.select_dtypes(include=['number'])\n",
        "\n",
        "# Impute missing values with mean\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "numerical_data_imputed = imputer.fit_transform(numerical_data_filtered)\n",
        "numerical_data_imputed_df = pd.DataFrame(numerical_data_imputed, columns=numerical_data_filtered.columns)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(numerical_data_imputed_df)\n",
        "scaled_data_df = pd.DataFrame(scaled_data, columns=numerical_data_imputed_df.columns)\n",
        "\n",
        "# Prepare your features and target variable, excluding 'Draft Year' from features for the model\n",
        "features = scaled_data_df.drop(columns=['WS', 'WS/48', 'BPM', 'VORP', 'VORP/48'])\n",
        "targets_ws48 = scaled_data_df['VORP/48']\n",
        "\n",
        "def create_model(input_shape, dropout_rate=0.6, l2_lambda=0.01):\n",
        "    model = Sequential([\n",
        "        Dense(32, activation='relu', input_shape=(input_shape,), kernel_regularizer=l2(l2_lambda)),  # Reduced from 64 to 32\n",
        "        Dropout(dropout_rate),  # Increased dropout rate\n",
        "        Dense(16, activation='relu', kernel_regularizer=l2(l2_lambda)),  # Reduced from 32 to 16, simplifying the model\n",
        "        Dropout(dropout_rate),  # Consistent dropout rate\n",
        "        Dense(1)  # Output layer remains the same\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Convert features and targets to numpy arrays for cross-validation\n",
        "X = features.to_numpy()\n",
        "y = targets_ws48.to_numpy().ravel()\n",
        "\n",
        "# Define Repeated K-Fold Cross Validator\n",
        "rkf = RepeatedKFold(n_splits=10, n_repeats=5, random_state=42)\n",
        "\n",
        "mse_scores = []\n",
        "rmse_scores = []\n",
        "r2_scores = []\n",
        "train_mse_scores = []\n",
        "train_rmse_scores = []\n",
        "train_r2_scores = []\n",
        "\n",
        "# Cross-validation loop\n",
        "for train_index, test_index in rkf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    model = create_model(input_shape=X_train.shape[1])\n",
        "    # Epochs are how many times over you go through\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0)\n",
        "\n",
        "    # Training set evaluation\n",
        "    train_predictions = model.predict(X_train)\n",
        "    train_mse = mean_squared_error(y_train, train_predictions.flatten())\n",
        "    train_rmse = np.sqrt(train_mse)\n",
        "    train_r2 = r2_score(y_train, train_predictions.flatten())\n",
        "    train_mse_scores.append(train_mse)\n",
        "    train_rmse_scores.append(train_rmse)\n",
        "    train_r2_scores.append(train_r2)\n",
        "\n",
        "    # Testing set evaluation\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions.flatten())\n",
        "    rmse = np.sqrt(mse)\n",
        "    r_squared = r2_score(y_test, predictions.flatten())\n",
        "    mse_scores.append(mse)\n",
        "    rmse_scores.append(rmse)\n",
        "    r2_scores.append(r_squared)\n",
        "\n",
        "# Compute average metrics for both training and testing sets\n",
        "print(f\"Average Train MSE: {np.mean(train_mse_scores)}\")\n",
        "print(f\"Average Train RMSE: {np.mean(train_rmse_scores)}\")\n",
        "print(f\"Average Train R-squared: {np.mean(train_r2_scores)}\")\n",
        "print(f\"Average Test MSE: {np.mean(mse_scores)}\")\n",
        "print(f\"Average Test RMSE: {np.mean(rmse_scores)}\")\n",
        "print(f\"Average Test R-squared: {np.mean(r2_scores)}\")"
      ],
      "metadata": {
        "id": "rteDLY9WMDV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters to test\n",
        "learning_rates = [0.01, 0.001, 0.0001]\n",
        "l2_lambdas = [0.01, 0.001, 0.0001]\n",
        "dropout_rates = [0.3, 0.5, 0.7]\n",
        "\n",
        "best_r2 = -float('inf')\n",
        "best_params = {}\n",
        "\n",
        "# Cross-validation loop with hyperparameter grid\n",
        "for lr in learning_rates:\n",
        "    for l2 in l2_lambdas:\n",
        "        for dropout in dropout_rates:\n",
        "            print(f\"Testing LR={lr}, L2={l2}, Dropout={dropout}\")\n",
        "            r2_scores = []\n",
        "\n",
        "            for train_index, test_index in rkf.split(X):\n",
        "                X_train, X_test = X[train_index], X[test_index]\n",
        "                y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "                model = Sequential([\n",
        "                    Dense(64, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(l2)),\n",
        "                    Dropout(dropout),\n",
        "                    Dense(32, activation='relu', kernel_regularizer=l2(l2)),\n",
        "                    Dropout(dropout),\n",
        "                    Dense(1)\n",
        "                ])\n",
        "                model.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
        "\n",
        "                model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0)\n",
        "\n",
        "                predictions = model.predict(X_test)\n",
        "                r_squared = r2_score(y_test, predictions.flatten())\n",
        "                r2_scores.append(r_squared)\n",
        "\n",
        "            average_r2 = np.mean(r2_scores)\n",
        "            print(f\"Average Test R-squared: {average_r2}\")\n",
        "\n",
        "            if average_r2 > best_r2:\n",
        "                best_r2 = average_r2\n",
        "                best_params = {'learning_rate': lr, 'l2_lambda': l2, 'dropout_rate': dropout}\n",
        "\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "j10aKzxp6wkr",
        "outputId": "e0cbb0e4-cec6-4bb1-e5a0-cb454149e6bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow.keras.wrappers'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-d294b7af3d01>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscikit_learn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.wrappers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}
