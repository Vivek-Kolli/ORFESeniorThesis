{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "metadata": {
        "id": "BBksgVOsOORX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## needed libraries\n",
        "from urllib.request import urlopen, Request\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "### error checking package\n",
        "import pdb\n",
        "import re"
      ],
      "metadata": {
        "id": "k3gCkR4LOPNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('2023round2.html', 'r') as file:\n",
        "    html_content = file.read()\n",
        "\n",
        "soup = BeautifulSoup(html_content, 'html.parser')"
      ],
      "metadata": {
        "id": "sykKNQBpQClp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv"
      ],
      "metadata": {
        "id": "U5wM9Gb2aNXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "player_draft_analysis = []\n",
        "\n",
        "# Change this depending on which round you're scraping (1,31 for Round 1) (31, 61 for Round 2)\n",
        "for i in range(31, 61):\n",
        "  pick_div = soup.find('div', id=f'pick-{i}')\n",
        "\n",
        "  if pick_div:\n",
        "      # Find the <strong>Pre-Draft Analysis</strong> within pick_div\n",
        "      pre_draft_analysis_h2 = pick_div.find('h2', string='Pre-Draft Analysis')\n",
        "      pick_div_img = pick_div.find('img')['src']\n",
        "      athlete_id = pick_div_img.split('/')[-1].split('.')[0]\n",
        "\n",
        "      print(athlete_id)\n",
        "\n",
        "      # Change the year in the url depending on the year you're scraping\n",
        "      api_url = f\"https://site.web.api.espn.com/apis/site/v2/sports/basketball/nba/draft/athlete/{athlete_id}?season=2017&region=us&lang=en\"\n",
        "\n",
        "      # Send a GET request to the API endpoint\n",
        "      response = requests.get(api_url)\n",
        "\n",
        "      predraft = ''\n",
        "      postdraft = ''\n",
        "\n",
        "      data = response.json()\n",
        "      if 'analysis' in data:\n",
        "        for analysis in data['analysis']:\n",
        "          if analysis['type'] == 'predraft':\n",
        "            predraft = analysis['text']\n",
        "          if analysis['type'] == 'postdraft':\n",
        "            postdraft = analysis['text']\n",
        "\n",
        "        # Find the <strong>Strengths</strong> section\n",
        "        clean_predraft = BeautifulSoup(predraft, 'html.parser').get_text(separator=\"\\n\")\n",
        "        clean_postdraft = BeautifulSoup(postdraft, 'html.parser').get_text(separator=\"\\n\")\n",
        "\n",
        "        # Find the <strong>Improvement areas</strong> section\n",
        "        #improvement_areas_section = soup.find('strong', string='Improvement areas').find_next_sibling()\n",
        "\n",
        "        player_draft_analysis.append({\n",
        "            'name': data['displayName'],\n",
        "            'predraft': clean_predraft,\n",
        "            'postdraft': clean_postdraft\n",
        "        })\n",
        "\n",
        "# Define the CSV file path and name\n",
        "csv_file_path = '2017round2.csv'\n",
        "\n",
        "# Define the field names (column headers) based on the keys in the dictionary\n",
        "field_names = ['name', 'predraft', 'postdraft']\n",
        "\n",
        "# Open the CSV file in write mode\n",
        "with open(csv_file_path, 'w', newline='') as csv_file:\n",
        "    # Create a CSV writer object\n",
        "    csv_writer = csv.DictWriter(csv_file, fieldnames=field_names)\n",
        "\n",
        "    # Write the header row\n",
        "    csv_writer.writeheader()\n",
        "\n",
        "    # Write each dictionary as a row in the CSV file\n",
        "    csv_writer.writerows(player_draft_analysis)\n",
        "\n",
        "print(f\"CSV file '{csv_file_path}' created successfully.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "p3UGrOK5S0Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SCRAPER FOR 2022 AND 2023, GETTING ONLY MOST RECENT PARAGRAPH\n",
        "\n",
        "player_draft_analysis = []\n",
        "\n",
        "# Change this depending on which round you're scraping (1,31 for Round 1) (31, 61 for Round 2)\n",
        "for i in range(32, 59):\n",
        "  pick_div = soup.find('div', id=f'pick-{i}')\n",
        "\n",
        "  if pick_div:\n",
        "      # Find the <strong>Pre-Draft Analysis</strong> within pick_div\n",
        "      pre_draft_analysis_h2 = pick_div.find('h2', string='Pre-Draft Analysis')\n",
        "      pick_div_img = pick_div.find('img')['src']\n",
        "      athlete_id = pick_div_img.split('/')[-1].split('.')[0]\n",
        "\n",
        "      print(i, athlete_id)\n",
        "\n",
        "\n",
        "\n",
        "      # These were only for the 2022 draft picks that didn't have a picture\n",
        "      if athlete_id == \"nophoto\":\n",
        "        if i == 55:\n",
        "          athlete_id = 105830\n",
        "        elif i == 56:\n",
        "          athlete_id = 106302\n",
        "\n",
        "      # Change the year in the url depending on the year you're scraping\n",
        "\n",
        "      api_url = f\"https://site.web.api.espn.com/apis/site/v2/sports/basketball/nba/draft/athlete/{athlete_id}?season=2022&region=us&lang=en\"\n",
        "\n",
        "      # Send a GET request to the API endpoint\n",
        "      response = requests.get(api_url)\n",
        "\n",
        "      most_recent_predraft = \"\"\n",
        "      postdraft = ''\n",
        "\n",
        "      data = response.json()\n",
        "      predraft_analysis = []\n",
        "      if 'analysis' in data:\n",
        "        for analysis in data['analysis']:\n",
        "          if analysis['type'] == 'predraft' and len(analysis['text']) > 0:\n",
        "            predraft = analysis['text']\n",
        "\n",
        "\n",
        "            curr_element = pick_div.find('b')\n",
        "            while curr_element.next_sibling:\n",
        "              curr_element = curr_element.next_sibling\n",
        "              if curr_element.name == \"b\":\n",
        "                  break\n",
        "              else:\n",
        "                  predraft_analysis.append(str(curr_element))\n",
        "\n",
        "      predraft_analysis = \"\".join(predraft_analysis)\n",
        "\n",
        "      clean_predraft = BeautifulSoup(predraft_analysis, 'html.parser').get_text(separator=\"\\n\")\n",
        "\n",
        "      if 'code' in data:\n",
        "        continue\n",
        "\n",
        "      player_draft_analysis.append({\n",
        "          'name': data['displayName'] ,\n",
        "          'predraft': clean_predraft,\n",
        "      })\n",
        "\n",
        "# Define the CSV file path and name\n",
        "csv_file_path = '2023round2test.csv'\n",
        "\n",
        "# Define the field names (column headers) based on the keys in the dictionary\n",
        "field_names = ['name', 'predraft']\n",
        "\n",
        "# Open the CSV file in write mode\n",
        "with open(csv_file_path, 'w', newline='') as csv_file:\n",
        "    # Create a CSV writer object\n",
        "    csv_writer = csv.DictWriter(csv_file, fieldnames=field_names)\n",
        "\n",
        "    # Write the header row\n",
        "    csv_writer.writeheader()\n",
        "\n",
        "    # Write each dictionary as a row in the CSV file\n",
        "    csv_writer.writerows(player_draft_analysis)\n",
        "\n",
        "print(f\"CSV file '{csv_file_path}' created successfully.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "qdI8iHsUnvCd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}